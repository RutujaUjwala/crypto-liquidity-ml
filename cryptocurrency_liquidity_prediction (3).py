# -*- coding: utf-8 -*-
"""Cryptocurrency Liquidity Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1opUqWBM412WcUI761khxFMZe1zAizCSU

# Project Overview: Cryptocurrency Liquidity Prediction

**This project aims to build a machine learning model to predict cryptocurrency liquidity levels based on various market factors. Below is a structured breakdown of the project requirements, steps, and expected deliverables.**

**Life cycle of Machine learning Project**

* Understanding the Problem Statement
* Data Collection
* Data Cleaning
* Exploratory data analysis
* Data Pre-Processing
* Model Training
* Choose best model

# ***1. Problem Statement***



1.Objective: Predict cryptocurrency liquidity levels to detect liquidity crises early.

2.Importance: Helps traders and exchange platforms manage risks effectively.

3.Market Factors: Trading volume, transaction patterns, exchange listings, social media activity.

# **2. Data Preprocessing Required**

Handle Missing Values: Identify and fill or remove missing data.
Normalize and Scale: Standardize numerical features for better model performance.

Feature Engineering: Create new features related to market liquidity trends.

**3. Dataset Information**

Source: Historical cryptocurrency price and trading volume data.

Timeframe: Records from 2022 march 16 & 17.

# ***4.Data Collection***
Dataset Link:
https://drive.google.com/drive/folders/10BRgPip2Zj_56is3DilJCowjfyT6E9AM)

### 4.1 Import Data and Required Packages
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime as dt
import seaborn as sns
import plotly.express as px
import warnings

warnings.filterwarnings("ignore")

# %matplotlib inline

# Load the dataset
file_2016 ='/content/coin_gecko_2022-03-16.csv'
file_2017 ='//content/coin_gecko_2022-03-17.csv' # Update with your file path
 # Update with your file path
data_2016 = pd.read_csv(file_2016)
data_2017 = pd.read_csv(file_2017)

# Display the first few rows of the dataset
data_2016.head()

# 1. Concatenate the DataFrames vertically
crypto_data = pd.concat([data_2016, data_2017], ignore_index=True)
crypto_data

df=pd.read_csv('/content/crypto_data.csv')

df.head()

df.shape

df.dtypes

# Check Null and Dtypes
df.info()

"""# **5 Exporing data**"""

df['date'].dtypes

df.columns

"""# **6.Feature Information:**

**coin**	:The name of the cryptocurrency (e.g., Bitcoin, Ethereum) data is of 16 and 17 ,year 2022 and month is march .

---


**symbol**	:The abbreviated symbol or ticker for the cryptocurrency (e.g., BTC, ETH) data is of 16 and 17 ,year 2022 and month is march .

---



**price**:The current price of the cryptocurrency in a specific currency (e.g., USD) data is of 16 and 17 ,year 2022 and month is march .

---



**1h**	:The percentage change in the price of the cryptocurrency over the past 1 hour data is of 16 and 17 ,year 2022 and month is march .

---



**24h**	:The percentage change in the price of the cryptocurrency over the past 24 hours data is of 16 and 17 ,year 2022 and month is march .

---



**7d**	:The percentage change in the price of the cryptocurrency over the past 7 days data is of 16 and 17 ,year 2022 and month is march .

---



**24h_volume**	:The total trading volume of the cryptocurrency in the past 24 hours data is of 16 and 17 ,year 2022 and month is march .

---



**mkt_cap**	:The market capitalization of the cryptocurrency (calculated as price multiplied by the total number of coins in circulation) data is of 16 and 17 ,year 2022 and month is march .

---



**date**	:The date and time the data was recorded data is of 16 and 17 ,year 2022 and month is march .

---

# **Data Preprocessing**
"""

df.columns

df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')

#let's check the datatype
df['date']

# define nuerical & categorical columns
#Let's drop the ID column

columns = [column for column in df.columns ]


numeric_features = [feature for feature in columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in columns if df[feature].dtype == 'O']

# print columns

print('We have {} numeric_features: {}'.format(len(numeric_features), numeric_features))

print('\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))

df.dtypes

df[categorical_features].value_counts().count()

# proportion of count data on categorical columns
for col in categorical_features:
  print(df[col].value_counts())

  print('---------------------------')

# Check for missing values
print("\nMissing Values in Combined Data:")
print(df.isnull().sum())

print('---------------------------')

# Check data types
print("\nData Types in Combined Data:")
print(df.dtypes)

print("\nMissing Values in df:")
print(df.isnull().sum())

df.describe()

"""# **Data Cleaning**

* Handling Missing values
* Handling Duplicates
* Check data type
* Understand the dataset

# Handling Missing values
"""

##these are the features with nan value
features_with_na=[features for features in df.columns if df[features].isnull().sum()>=1]
for feature in features_with_na:
    print(feature,np.round(df[feature].isnull().mean()*100,5), '% missing values')

# statistics on numerical columns (Null cols)
df[features_with_na].select_dtypes(exclude='object').describe()

"""# **Imputing Null Values**

Using the mean to fill missing values in Trading Volume helps maintain the overall average of the dataset, minimizing the impact on statistical analyses. This method is simple and effective, especially when the data is normally distributed and the missing values are random
"""

df['1h'].fillna(df['1h'].mean(), inplace=True)
df['24h'].fillna(df['24h'].mean(), inplace=True)
df['7d'].fillna(df['7d'].mean(), inplace=True)
df['24h_volume'].fillna(df['24h_volume'].mean(), inplace=True)

print("\nPreprocessed Data Overview:")
df[features_with_na].select_dtypes(exclude='object').describe()

"""# ***Handling Duplicates***"""

df.dtypes

df.duplicated().sum()

# Extract and assign the date
df['date'] = df['date'].dt.day
# because this data is talking about only 2022 march date 16 and 17 with same date and month
# which are 3

"""# EDA"""

# Number of cryptocurrencies listed
#latest_df.shape
print("Number of cryptocurrencies listed")
df['symbol'].nunique()

# Number of cryptocurrencies listed
#latest_df.shape
print("Number of cryptocurrencies listed")
df['coin'].nunique()

"""#  Numerical Features"""

plt.figure(figsize=(15, 10))
plt.suptitle('Univariate Analysis of Numerical Features', fontsize=20, fontweight='bold', alpha=0.8, y=1.)

for i in range(0, len(numeric_features)):
    plt.subplot(3, 3, i+1)
    sns.kdeplot(x=df[numeric_features[i]], color='blue')
    plt.xlabel(numeric_features[i])
    plt.tight_layout()

# save plot
# plt.savefig('./images/Univariate_Num.png')

"""# Univariate Analysis Report: Cryptocurrency Data Methodology:







`The following numerical features were analyzed:`

**`price`**: The distribution of cryptocurrency prices appears to be right-skewed, indicating a higher concentration of lower-priced cryptocurrencies with a few outliers at significantly higher prices.

**`1h`**: This feature, representing the percentage change in price over the past hour, exhibits a roughly normal distribution centered around 0. This suggests that hourly price fluctuations are typically balanced between positive and negative changes.

**`24h`**: The distribution of the percentage change in price over the past 24 hours also shows a generally normal distribution but with a wider spread compared to the 1-hour change, indicating greater price volatility over a longer timeframe.

**`7d`**: The distribution of the percentage change in price over the past 7 days is similar to the 24-hour change, though potentially with a slightly wider spread, reflecting even greater price fluctuation over a week.

**`24h_volume`**: The distribution of 24-hour trading volume is likely right-skewed, indicating that a few cryptocurrencies have significantly higher trading volumes compared to the majority.

**`mkt_cap`**: The market capitalization distribution is also expected to be right-skewed, suggesting a concentration of lower market cap cryptocurrencies with a few outliers having much larger market caps.



# Overall Observations:

Several numerical features, including price, 24h_volume, and mkt_cap, show signs of right-skewness, indicating the presence of outliers with extremely high values.
Percentage change features (1h, 24h, 7d) tend to follow normal distributions, suggesting balanced price fluctuations over different time horizons.
Understanding the distributions of these individual features is crucial for further data preprocessing and model building.
Next Steps:

#  Market Capitalization
This approach focuses on selecting the cryptocurrencies with the highest historical market capitalization, providing a ranking that isn't limited to a single date within the limited timeframe of your dataset

# **How is it calculated?**

Market Capitalization = Current Market Price per Unit * Total Number of Units Outstanding
# For cryptocurrencies, it's calculated as

Market Capitalization = Price of One Coin * Total Number of Coins in Circulation
"""

df.sort_values(by=['mkt_cap'], ascending=[False])
df[['coin','mkt_cap','price']].head(10)

"""**`Bitcoin and Ethereum`** likely dominated the market in 2016-2017, with significant price volatility and trading volume driving market trends.

**`Emerging cryptocurrencies experienced rapid growth`**, offering potential investment opportunities but with higher risk profiles.

**`Market sentiment and external factors influenced price movements`**, highlighting the importance of external data for comprehensive analysis.

# Distribution plots of key numerical features
"""

# Plot settings
sns.set(style="whitegrid")
plt.figure(figsize=(16, 10))

# 1. Histograms for numeric columns
fig, axs = plt.subplots(2, 3, figsize=(18, 10))
numeric_to_plot = ['price', '1h', '24h', '7d', '24h_volume', 'mkt_cap']

for ax, col in zip(axs.flat, numeric_to_plot):
    sns.histplot(df[col], kde=True, ax=ax, bins=30, color='green')
    ax.set_title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

"""**`Price, Market Cap, and Volume`** are heavily right-skewed, with a few very high values.

**`1h, 24h, and 7d %`** changes are centered around zero, as expected in short-term crypto fluctuations.
"""

numeric_features = [feature for feature in numeric_features if feature != 'date']

# Calculate the correlation matrix
correlation_matrix = df[numeric_features].corr()

# Create the heatmap
plt.figure(figsize=(8, 8))  # Adjust size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='RdBu', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()
plt.show()

"""` Price and Market Cap`: They move together, but some coins mess it up (maybe due to large total supply).

 `Short-term Momentum`: Coins that move today tend to continue for a few days (not random, slight trend persistence).

` Big Coins Dominate Volume`: Market cap and 24h volume are tightly linked.

 `Price Level ≠ Price Change`: Expensive or cheap, any coin can spike/drop.


"""

# Group by date and calculate mean for price, market cap, and 24h volume
mean_price = df.groupby('date')['price'].mean()
mean_market_cap = df.groupby('date')['mkt_cap'].mean()
mean_volume = df.groupby('date')['24h_volume'].mean()

# Set up the subplots
fig, axs = plt.subplots(3, 1, figsize=(14, 8))

# Plot Mean Price
axs[0].plot(mean_price.index, mean_price.values, marker='o', color='blue')
axs[0].set_title('Mean Cryptocurrency Price Over Time')
axs[0].set_xlabel('Date')
axs[0].set_ylabel('Mean Price')

# Plot Mean Market Cap
axs[1].plot(mean_market_cap.index, mean_market_cap.values, marker='o', color='green')
axs[1].set_title('Mean Market Capitalization Over Time')
axs[1].set_xlabel('Date')
axs[1].set_ylabel('Mean Market Cap')

# Plot Mean 24h Volume
axs[2].plot(mean_volume.index, mean_volume.values, marker='o', color='orange')
axs[2].set_title('Mean 24h Trading Volume Over Time')
axs[2].set_xlabel('Date')
axs[2].set_ylabel('Mean 24h Volume')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""**`Visualization Analysis`**: I've created three time-series charts showing:

Mean cryptocurrency price trends

Mean market capitalization evolution

Mean 24-hour trading volume patterns

These charts provide visual representation of price movements, market cap changes, and trading volume fluctuations over time, helping to identify trends and patterns in the cryptocurrency market.

Each metric is plotted against time with clear markers and grid lines for better readability. The blue, green, and orange lines represent price, market cap, and trading volume respectively.




"""

# Sort by price first, then by market cap in descending order
top_currencies_by_price_and_mkt_cap = df.sort_values(by=['price', 'mkt_cap'], ascending=[False, False])

# Display the updated DataFrame
top_10=top_currencies_by_price_and_mkt_cap[['coin', 'price', 'mkt_cap']].head(10)


top_10

#the top_10 DataFrame as described

# Create the bar graph for Price
plt.figure(figsize=(13, 9))
plt.bar(top_10['coin'], top_10['price'], label='Price')

# Create a secondary axis for Market Cap
plt.twinx()  # Create a shared x-axis
plt.bar(top_10['coin'], top_10['mkt_cap'], color='blue', alpha=0.5, label='Market Cap')

plt.title('Top 10 Cryptocurrencies by Price and Market Cap')
plt.xlabel('Coin')
plt.ylabel('Price')  # Label for the primary y-axis
plt.xticks()
plt.tight_layout()
plt.legend(loc='upper right') # Add a legend to differentiate the bars
plt.show()



"""# **Feature Engineering:**

-   liquidity_ratio = 24h_volume / mkt_cap
- price_change = price_today - price_yesterday
- volume_change = volume_today - volume_yesterday
- Create "Top Coin" flag .
"""

# Simple liquidity ratio
df['liquidity_ratio'] = (df['24h_volume'] / df['mkt_cap']+ 1e-6)
df.head()

# Price change over 2 days
df['price_change'] = (df['price'].diff()+1e-6)
df.head()

df['price_change'].fillna(0, inplace=True  )

# Volume change over 2 days
df['volume_change'] = (df['24h_volume'].diff()+1e-6)

df['volume_change'].fillna(0, inplace=True  )

df.head()

df.drop('symbol', axis=1, inplace=True)

df.head()

# Group coins based on market cap
def categorize_coins(market_cap):
    if market_cap >= df['mkt_cap'].quantile(0.95):  # Top 5%
        return 'Top 5 Coins'
    elif market_cap >= df['mkt_cap'].quantile(0.8):  # 20%
        return 'Large-caps'
    elif market_cap >= df['mkt_cap'].quantile(0.5): # Between 50% and 80%
        return 'Mid-caps'
    else:
       return 'small-caps'  # Bottom 50%

df['coin_category'] = df['mkt_cap'].apply(categorize_coins)

# Display the updated DataFrame
df.head()

#  drop any infinite values from division
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)

df.coin_category.value_counts()

df['coin_category'].value_counts().plot(kind='bar', title='Coin Category Distribution')

df["coin_category"].replace({"small-caps":0,"Mid-caps":1, "Large-caps":2, "Master":3, "Top 5 Coins":4},inplace=True)

df.drop('coin', axis=1, inplace=True)

"""# **Why are we capping it and why not trim it ?**
* As you see this data is only of 2 days 16 and 17 Trimming outliers may result in the removal of a large number of records from this dataset as we have already very less rows so this isn’t desirable in this case since columns other than the ones containing the outlier values may contain useful information.

* In this cases, you can use outlier capping to replace the outlier values with a maximum or minimum capped values. Be warned, this manipulates our data but we can replace outlier values by the upper and lower limit calculated using the IQR range.

**Outlier detection**
"""

# define numerical & discrete features
columns = [column for column in df.columns]

numeric_features_updated = [feature for feature in columns if df[feature].dtype in ['float64', 'float32']]
discrete_features = [feature for feature in columns if df[feature].dtype in ['int64', 'int32']]


print('We have {} numeric_features: {}'.format(len(numeric_features), numeric_features))
print('\nWe have {} discrete_features : {}'.format(len(discrete_features), discrete_features))

num_features = numeric_features_updated.copy()
plt.figure(figsize=(25, 10))

for i in range(len(num_features)):
        plt.subplot(2,6,i+1)
        sns.set_style('ticks')
        plt.xlabel(num_features[i])
        ax = sns.boxplot(df[num_features[i]])

"""**report**

-  there are outliers in following columns -
    - Price
    - 1hr
    - 24hr
    - 7d
    - 24h_volume
    - mkt_cap
    - liquidity ratio
    - price-change
    - volume_change
    
"""

def detect_outliers(col):
    # Finding the IQR
    percentile25 = df[col].quantile(0.25)
    percentile75 = df[col].quantile(0.75)
    print('\n ####', col , '####')
    print("percentile25",percentile25)
    print("percentile75",percentile75)
    iqr = percentile75 - percentile25
    upper_limit = percentile75 + 1.5 * iqr
    lower_limit = percentile25 - 1.5 * iqr
    print("Upper limit",upper_limit)
    print("Lower limit",lower_limit)
    df.loc[(df[col]>upper_limit), col]= upper_limit
    df.loc[(df[col]<lower_limit), col]= lower_limit
    return df

for col in numeric_features_updated:
         detect_outliers(col)

df[numeric_features_updated].skew(axis=0, skipna=True)

from statsmodels.stats.outliers_influence import variance_inflation_factor
def compute_vif(considered_features, df):

    X = df[considered_features]
    # the calculation of variance inflation requires a constant
    X['intercept'] = 1

    # create dataframe to store vif values
    vif = pd.DataFrame()
    vif["Variable"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif = vif[vif['Variable']!='intercept']
    return vif

# Exclude 'coin_category' from VIF calculation
cont_features = ['price', '1h', '24h', '7d', '24h_volume', 'mkt_cap', 'liquidity_ratio', 'price_change', 'volume_change']
# Removed 'coin_category'

# we will not chech vif for coin and coin category
vif_data  =compute_vif(cont_features, df)
vif_data

""" Most research papers consider a VIF (Variance Inflation Factor) > 10 as an indicator of multicollinearity, but some choose a more conservative threshold of 5.

**As we can see the Vif for the columns are less than 5, we can safely assume that the data has not such correlations.
"""

plt.figure(figsize = (10,8))
num_features =cont_features.copy()

sns.heatmap(df[num_features].corr(), annot=True, cmap="YlGnBu")
plt.show()

"""# Model Selection:

✅ 2.** Pick Baseline Models **



**Model:**

Linear Regression

Random Forest Regressor

XGBoost Regressor	Regression-->	High performance, handles feature interactions

Decision Tree Regressor
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score



# Load data
data = df.copy()



# Define features and target
features = ['price', '1h', '24h', '7d', '24h_volume', 'mkt_cap', 'date', 'price_change', 'volume_change', 'coin_category']
X = data[features]
y = data['liquidity_ratio']





# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale only after splitting
scaler =  StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest Regressor": RandomForestRegressor(random_state=42),
    "Decision Tree Regressor": DecisionTreeRegressor(random_state=42),
    "XGBoost Regressor": XGBRegressor(random_state=42)
}

# Store results
results = {}

# Train and evaluate regression models
for name, model in models.items():
     model.fit(X_train_scaled, y_train)
     y_pred = model.predict(X_test_scaled)
     results[name] = {
        "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
        "MAE": mean_absolute_error(y_test, y_pred),
        "R²": r2_score(y_test, y_pred)

        }


# Display results
results_data = pd.DataFrame(results).T
styled_results = results_data.style.background_gradient(cmap='Blues') \
    .set_caption('Model Performance Comparison') \
    .format("{:.4f}")  # Optional: format numbers to 4 decimal places

display(styled_results)

"""**📌 Footnote: Interpreting Model Performance**

RMSE (Root Mean Squared Error): Measures how far predictions are from actual values — lower is better.

MAE (Mean Absolute Error): Shows average absolute error between predicted and actual values — also lower is better.

R² (R-squared): Indicates how much variance in liquidity_ratio is explained by the model — closer to 1.0 is better.

**Random Forest--->	✅ Best overall — lowest error, highest R²**

**XGBoost Regressor	--->🔹 Close second, still very strong**
"""

from sklearn.pipeline import Pipeline
# Loop through models again to collect predictions
for name, model in models.items():
    pipe = Pipeline([ ('model', model)])
    pipe.fit(X_train_scaled, y_train)
    y_pred = pipe.predict(X_test_scaled)

    # Plot actual vs predicted
    plt.figure(figsize=(6, 5))

    plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # perfect prediction line
    plt.title(f"{name}: Actual vs Predicted")
    plt.xlabel("Actual Liquidity Ratio")
    plt.ylabel("Predicted Liquidity Ratio")
    plt.grid(True)
    plt.tight_layout()
    plt.show()  # Indented to be inside the loop

"""# **✅ Ways to Improve R² Random Forest**

🔧 Hyperparameter Tuning

🔹 XGBoost:
"""

param_grid = {
         'n_estimators': [100, 200, 300,320, 500],
         'max_depth': [20, 21,22,23,24,30, None],
         'min_samples_split': [2, 3,5, 10,11,12,20],
         'min_samples_leaf': [1, 2, 3,4,5,6,8],
         'max_features': ['auto', 'sqrt', 'log2',None],
         'bootstrap': [True, False]
     }

from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assuming param_grid is already defined

# Initialize Random Forest and KFold
rf = RandomForestRegressor(random_state=42,max_depth=20, min_samples_split=5)
cv = KFold(n_splits=12, shuffle=True, random_state=42)

# Randomized Search CV
search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,
                            n_iter=50, cv=cv, scoring='r2',
                            random_state=42, n_jobs=-1)

# Fit the RandomizedSearchCV object
search.fit(X_train_scaled, y_train)

# Get the best estimator
best_rf = search.best_estimator_

# Predict and evaluate
y_pred = best_rf.predict(X_test_scaled)

print("Best Params:", search.best_params_)
print("Improved R²:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("MAE:", mean_absolute_error(y_test, y_pred))

residuals = y_test - y_pred

plt.figure(figsize=(8, 5))
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residual Plot")
plt.xlabel("Predicted Liquidity Ratio")
plt.ylabel("Residuals")
plt.grid(True)
plt.tight_layout()
plt.show()

import pickle
# Get the best estimator
best_rf = search.best_estimator_

# Save the model using pickle
with open('best_rf_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)